{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "documents = []\n",
    "with open(\"resumes.json\") as fp: \n",
    "    for line in fp: \n",
    "        doc_json = json.loads(line)\n",
    "        documents.append(doc_json['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "api_key = os.getenv('AZURE_API_KEY')\n",
    "credential = AzureKeyCredential(api_key)\n",
    "endpoint=\"https://astrok2-ner-test.cognitiveservices.azure.com/\"\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for i, doc in enumerate(documents):\n",
    "    with open('results/azure/' + str(i) + \".json\", \"w\") as f:\n",
    "        if len(doc) > 5120:\n",
    "            start_pos = 0\n",
    "            r_json = {'entities': []}\n",
    "            while start_pos + 5120 < len(doc):\n",
    "                end_pos = start_pos + doc[start_pos:(start_pos+5120)].rfind(' ')\n",
    "                response = text_analytics_client.recognize_entities([doc[start_pos:end_pos]], language=\"en\")\n",
    "                [r_json['entities'].append({'text': e.text, 'category': e.category, 'confidence_score': e.confidence_score}) for e in response[0].entities]\n",
    "                start_pos = end_pos\n",
    "            response = text_analytics_client.recognize_entities([doc[end_pos:]], language=\"en\")\n",
    "            [r_json['entities'].append({'text': e.text, 'category': e.category, 'confidence_score': e.confidence_score}) for e in response[0].entities]\n",
    "            json.dump(r_json, f)\n",
    "        else:\n",
    "            response = text_analytics_client.recognize_entities([doc], language=\"en\")\n",
    "            r = response[0]\n",
    "            r_json = {'entities': [{'text': e.text, 'category': e.category, 'confidence_score': e.confidence_score} for e in r.entities]}\n",
    "            json.dump(r_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling DetectEntities\n",
      "{\n",
      "    \"Entities\": [\n",
      "        {\n",
      "            \"BeginOffset\": 14,\n",
      "            \"EndOffset\": 19,\n",
      "            \"Score\": 0.9925345182418823,\n",
      "            \"Text\": \"today\",\n",
      "            \"Type\": \"DATE\"\n",
      "        },\n",
      "        {\n",
      "            \"BeginOffset\": 23,\n",
      "            \"EndOffset\": 30,\n",
      "            \"Score\": 0.9988563060760498,\n",
      "            \"Text\": \"Seattle\",\n",
      "            \"Type\": \"LOCATION\"\n",
      "        }\n",
      "    ],\n",
      "    \"ResponseMetadata\": {\n",
      "        \"HTTPHeaders\": {\n",
      "            \"content-length\": \"200\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"date\": \"Sun, 18 Oct 2020 00:32:06 GMT\",\n",
      "            \"x-amzn-requestid\": \"52d7335f-1882-4ab5-8d4e-e5db60e3bbfd\"\n",
      "        },\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"RequestId\": \"52d7335f-1882-4ab5-8d4e-e5db60e3bbfd\",\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n",
      "End of DetectEntities\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    with open('results/amazon/' + str(i) + \".json\", \"w\") as f:  \n",
    "        if len(bytes(doc, 'utf-8')) > 5000:\n",
    "            start_pos = 0\n",
    "            r_json = {'Entities': []}\n",
    "            while start_pos + 4850 < len(doc):\n",
    "                end_pos = start_pos + doc[start_pos:(start_pos+4850)].rfind(' ')\n",
    "                r = comprehend.detect_entities(Text=doc[start_pos:end_pos], LanguageCode='en')\n",
    "                [r_json['Entities'].append(e) for e in r['Entities']]\n",
    "                start_pos = end_pos\n",
    "            r = comprehend.detect_entities(Text=doc[end_pos:], LanguageCode='en')\n",
    "            [r_json['Entities'].append(e) for e in r['Entities']]\n",
    "            json.dump(r_json, f)\n",
    "        else:\n",
    "            r_json = comprehend.detect_entities(Text=doc, LanguageCode='en')\n",
    "            json.dump(r_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Natural Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Build language API client (requires service account key)\n",
    "client = language.LanguageServiceClient.from_service_account_json('services.json')\n",
    "\n",
    "# Define functions\n",
    "def pull_googlenlp(client, text, invalid_types = ['OTHER'], **data):\n",
    "    document = types.Document(content=text, type=language.enums.Document.Type.PLAIN_TEXT)\n",
    "    features = {'extract_syntax': False, \n",
    "                'extract_entities': True, \n",
    "                'extract_document_sentiment': False, \n",
    "                'extract_entity_sentiment': False,\n",
    "                'classify_text': False\n",
    "               }\n",
    "    response = client.annotate_text(document=document, features=features)\n",
    "    entities = response.entities\n",
    "    def get_type(type):\n",
    "        return client.enums.Entity.Type(entity.type).name\n",
    "    result = {'entities': []}\n",
    "    for entity in entities:\n",
    "        if get_type(entity.type) not in invalid_types:\n",
    "            result['entities'].append({'name': entity.name, 'type': get_type(entity.type), 'salience': entity.salience})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    with open('results/google/' + str(i) + \".json\", \"w\") as f:\n",
    "        json.dump(pull_googlenlp(client, doc), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanford NLP Tagger via NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('classifiers/english.all.3class.distsim.crf.ser.gz', \n",
    "                       'stanford-ner-4.0.0.jar',\n",
    "                       encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def bio_chunking(classified_text):\n",
    "    chunks = []\n",
    "    prev_tag = \"O\"\n",
    "    chunk = None\n",
    "    for token, tag in classified_text:\n",
    "        if re.fullmatch(r'[\\W]+', token):\n",
    "            continue\n",
    "        if tag == \"O\":\n",
    "            prev_tag = tag\n",
    "            if chunk is not None:\n",
    "                chunks.append(chunk)\n",
    "                chunk = None\n",
    "        else:\n",
    "            if prev_tag == tag:\n",
    "                chunk = (chunk[0] + ' ' + token, tag)\n",
    "            else:\n",
    "                if chunk is not None:\n",
    "                    chunks.append(chunk)\n",
    "                chunk = (token, tag)\n",
    "                prev_tag = tag\n",
    "    if chunk is not None:\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    with open('results/stanford/' + str(i) + \".json\", \"w\") as f:  \n",
    "        tokenized_text = word_tokenize(doc)\n",
    "        classified_text = st.tag(tokenized_text)\n",
    "        classified_text = bio_chunking(classified_text)\n",
    "        r_json = {'entities': [{'text': e[0], 'category': e[1]} for e in classified_text]}\n",
    "        json.dump(r_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abhishek Jha', 'PERSON'),\n",
       " ('Karnataka', 'LOCATION'),\n",
       " ('Bangalore Karnataka', 'LOCATION'),\n",
       " ('Hubli Karnataka', 'LOCATION'),\n",
       " ('Mac Non', 'ORGANIZATION')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    with open('results/spacy/' + str(i) + \".json\", \"w\") as f:  \n",
    "        classified_doc = nlp(doc)\n",
    "        r_json = {'entities': [{'text': e.text, 'category': e.label_, 'start': e.start_char, 'end': e.end_char} for e in classified_doc.ents]}\n",
    "        json.dump(r_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
